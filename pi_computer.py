# -*- coding: utf-8 -*-
"""PI_computer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H_ZUUsi8xdIbAJU1ZBI1L3wEQ1gwhw1H
"""

import numpy as np
import numpy.random as npr
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import keras.backend as kb
import tensorflow as tf
tf.config.experimental_run_functions_eagerly(True)
import tensorflow.keras as keras
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras import Model
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import math
from keras import backend as K

from numpy import genfromtxt
datatr = genfromtxt('truth_4input_tr2.csv', delimiter=',')
datate = genfromtxt('truth_4input_te.csv', delimiter=',')
print(datatr.shape,datate.shape)

class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.flatten = Flatten()
    self.d1 = Dense(20, activation='relu')
    #self.dr1 = Dropout(0.5)
    #self.d2 = Dense(20, activation='relu')
    #self.d3 = Dense(20, activation='relu')
    self.d4 = Dense(2)

  def call(self, x):
    x = self.flatten(x)
    x = self.d1(x)
    #x = self.dr1(x)
    #x = self.d2(x)
    #x = self.dr1(x)
    #x = self.d3(x)
    return self.d4(x)

def customized_loss(y_true, y_pred, a2):
    a4=0.5
    y_true = tf.cast(y_true, dtype=tf.float32)
    y_pred0 = tf.expand_dims(y_pred[:,0],-1)
    y_pred1 = tf.expand_dims(y_pred[:,1],-1)
    y_pred_bigger=tf.nn.relu(y_pred0-y_pred1)+y_pred1
    y_pred_smaller=-tf.nn.relu(y_pred0-y_pred1)+y_pred0
    ksoft = 1 - tf.math.sigmoid( a4 * (y_pred_bigger- y_true)) * tf.math.sigmoid( a4 * (y_true- y_pred_smaller))
    cross_ent = (y_pred_bigger-y_pred_smaller) + a2 * ksoft

    if tf.math.is_nan(tf.math.reduce_mean(cross_ent)):
        print('1', (y_pred_bigger-y_pred_smaller))
        print('ksoft', ksoft)
        print('3', tf.math.log(y_correctp + eps))
        print('4', tf.math.log(y_wrongp + eps))
        print(' y_correctp',  y_correct)
        print(' exp y_correctp', tf.math.exp(y_correct))
        print(' y_correctp',  y_wrong)
        print(' exp y_correctp', tf.math.exp(y_wrongt))
    return tf.math.reduce_mean(cross_ent)



@tf.function
def train_step(images, labels, optimizer, model2, a2):

  with tf.GradientTape() as tape:
    predictions = model2(images)
    loss = customized_loss(labels, predictions, a2)

  gradients = tape.gradient(loss, model2.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model2.trainable_variables))
  return loss



def train_network(x_train, y_train, a2):
  EPOCHS = 40
  optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)
  model2 = MyModel()
  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(100).batch(1)
  for epoch in range(EPOCHS):

    for images, labels in train_ds:
      train_loss = train_step(images, labels, optimizer, model2, a2)

    train_loss = []
    for images, labels in train_ds:
      predictions = model2(images)
      loss_on_batch = customized_loss(labels, predictions, a2)
      train_loss.append(loss_on_batch)
    if epoch % 5 == 0:
      print("Epoch {}, Train loss: {}".format(epoch, tf.reduce_mean(train_loss)))
  return model2



def stats_result(list_min, list_max, y):
  Lval = np.mean(list_min, 0)
  Uval = np.mean(list_max,0)
  sum1=0
  for i in range(len(y)):
    if Lval[i]<=y[i]<=Uval[i]:
      sum1=sum1+1
  print('sum form the val', sum1/len(y))
  width_tr = np.mean(Uval-Lval)
  print('width from the val set',width_tr)
  return Lval,Uval,sum1/len(y), width_tr


num_lambda = np.arange(15,33,1)
numval=100

def model_generation(re, re2, re3, datatr, datate):
  for a2 in num_lambda:
    x_train= datatr[1:(1+re*numval),1:5]
    print(x_train.shape)
    y_train = datatr[1:(1+re*numval),5] *1000
    y_train=y_train.reshape([len(y_train),1])

    tf.compat.v1.get_default_graph
    model = train_network(x_train, y_train, a2)
    model.save('model'+str(a2))
  return 1


def dataset_generation1(re, re2, re3, i, datatr, datate):
  tr_list_min = []
  tr_list_max = []
  tr_list_conf = []
  val_list_min = []
  val_list_max = []
  test_list_min = []
  test_list_max = []
  x_val_tmp= datatr[1+re*numval:,(5*i+1):(5*i+5)]
  y_val_tmp = datatr[1+re*numval:,5*i+5] *1000
  y_val_tmp= y_val_tmp.reshape([len(y_val_tmp),1])
  x_val = np.zeros(shape=x_val_tmp.shape)
  y_val = np.zeros(shape=y_val_tmp.shape)
  for jj in range(numval):
    for jjj in range(re2):
      x_val[jj*re2+jjj]=x_val_tmp[jjj*numval+jj]
      y_val[jj*re2+jjj]=y_val_tmp[jjj*numval+jj]

  x_test= datate[1:,1:5]
  y_test = datate[1:,5] *1000
  y_test=y_test.reshape([len(y_test),1])

  for a2 in num_lambda:
    new_model = tf.keras.models.load_model('model'+str(a2), compile=False)
    pred_val_part=new_model.predict(x_val)
    pred_test_part=new_model.predict(x_test)
    val_list_min.append(np.min(pred_val_part,-1))
    val_list_max.append(np.max(pred_val_part,-1))
    test_list_min.append(np.min(pred_test_part,-1))
    test_list_max.append(np.max(pred_test_part,-1))
  sum1, sum2, width_tr, width_test=calibration1(val_list_min, val_list_max, test_list_min, test_list_max, y_val, y_test, re2)
  print(sum2, width_tr, width_test)
  return sum1, sum2, width_tr, width_test


def dataset_generation2(re, re2, re3, i, datatr, datate):
  tr_list_min = []
  tr_list_max = []
  tr_list_conf = []
  val_list_min = []
  val_list_max = []
  test_list_min = []
  test_list_max = []
  x_val_tmp= datatr[1+re*numval:,(5*i+1):(5*i+5)]
  y_val_tmp = datatr[1+re*numval:,(5*i+5)] *1000
  y_val_tmp= y_val_tmp.reshape([len(y_val_tmp),1])
  x_val = np.zeros(shape=x_val_tmp.shape)
  y_val = np.zeros(shape=y_val_tmp.shape)
  for jj in range(numval):
    for jjj in range(re2):
      x_val[jj*re2+jjj]=x_val_tmp[jjj*numval+jj]
      y_val[jj*re2+jjj]=y_val_tmp[jjj*numval+jj]


  x_test= datate[1:,1:5]
  y_test = datate[1:,5] *1000
  y_test=y_test.reshape([len(y_test),1])

  for a2 in num_lambda:
    new_model = tf.keras.models.load_model('model'+str(a2), compile=False)
    pred_val_part=new_model.predict(x_val)
    pred_test_part=new_model.predict(x_test)
    val_list_min.append(np.min(pred_val_part,-1))
    val_list_max.append(np.max(pred_val_part,-1))
    test_list_min.append(np.min(pred_test_part,-1))
    test_list_max.append(np.max(pred_test_part,-1))
  sum1, sum2, width_tr, width_test=calibration2(val_list_min, val_list_max, test_list_min, test_list_max, y_val, y_test, re2)
  print(sum2, width_tr, width_test)
  return sum1, sum2, width_tr, width_test

def dataset_generation3(re, re2, re3, i, datatr, datate):
  tr_list_min = []
  tr_list_max = []
  tr_list_conf = []
  val_list_min = []
  val_list_max = []
  test_list_min = []
  test_list_max = []
  x_val_tmp= datatr[1+re*numval:,(5*i+1):(5*i+5)]
  y_val_tmp = datatr[1+re*numval:,(5*i+5)] *1000
  y_val_tmp= y_val_tmp.reshape([len(y_val_tmp),1])
  x_val = np.zeros(shape=x_val_tmp.shape)
  y_val = np.zeros(shape=y_val_tmp.shape)
  for jj in range(numval):
    for jjj in range(re2):
      x_val[jj*re2+jjj]=x_val_tmp[jjj*numval+jj]
      y_val[jj*re2+jjj]=y_val_tmp[jjj*numval+jj]


  x_test= datate[1:,1:5]
  y_test = datate[1:,5] *1000
  y_test=y_test.reshape([len(y_test),1])

  for a2 in num_lambda:
    new_model = tf.keras.models.load_model('model'+str(a2), compile=False)
    pred_val_part=new_model.predict(x_val)
    pred_test_part=new_model.predict(x_test)
    val_list_min.append(np.min(pred_val_part,-1))
    val_list_max.append(np.max(pred_val_part,-1))
    test_list_min.append(np.min(pred_test_part,-1))
    test_list_max.append(np.max(pred_test_part,-1))
  sum1, sum2, width_tr, width_test=calibration3(val_list_min, val_list_max, test_list_min, test_list_max, y_val, y_test, re2)
  print(sum2, width_tr, width_test)
  return sum1, sum2, width_tr, width_test


margin=+0.0


def calibration2(list_min, list_max, test_list_min, test_list_max, y_val, y_test, re2): #NNGN
  print(len(list_min))
  nnnum=len(list_min)
  sum1=np.zeros(len(list_min))
  sum2=np.zeros(len(test_list_min))
  list_min=np.array(list_min)
  list_max=np.array(list_max)
  test_list_min=np.array(test_list_min)
  test_list_max=np.array(test_list_max)
  W1=np.zeros([nnnum,numval])
  for tt in range(nnnum):
    for i in range(numval):
      for j in range(re2):
        if list_min[tt,i*re2+j]-margin<=y_val[i*re2+j]<=list_max[tt,i*re2+j]+margin:
          W1[tt,i]=W1[tt,i]+1/re2
  covW1=np.cov(W1)
  meanW1=np.mean(W1,-1)
  print(meanW1.shape)
  print('sum form the val', meanW1)
  sigma_diagonal = [covW1[i,i] for i in range(nnnum)]
  sigma_diagonal = np.sqrt(sigma_diagonal)+0.001
  conf_level=0.95
  mean = np.zeros(nnnum)
  cov = covW1
  samples = np.max(np.random.multivariate_normal(mean, cov, 10000)/sigma_diagonal, axis = 1)
  cl_quantile = np.quantile(samples, conf_level)
  print('diagonal', sigma_diagonal)
  print('cl', cl_quantile)
  for tt in range(len(sum1)):
    for i in range(len(y_val)):
      if list_min[tt,i]-margin<=y_val[i]<=list_max[tt,i]+margin:
        sum1[tt]=sum1[tt]+1/len(y_val)
  for tt in range(len(sum2)):
    for i in range(len(y_test)):
      if test_list_min[tt,i]-margin<=y_test[i]<=test_list_max[tt,i]+margin:
        sum2[tt]=sum2[tt]+1/len(y_test)
  print('sum form the val', sum1, sum2)
  width_tr = np.mean(list_max-list_min,-1)+2*margin
  width_te = np.mean(test_list_max-test_list_min,-1)+2*margin
  print('width from the val set',width_tr,width_te)
  if 0 not in list(map(int, meanW1 <= 0.95+cl_quantile*sigma_diagonal/np.sqrt(numval))):
    index_val = list(meanW1).index(np.max(meanW1))
  else:
    min_value = np.min(width_tr[meanW1>0.95+cl_quantile*sigma_diagonal/np.sqrt(numval)])
    index_val = list(width_tr).index(min_value)
  print('check', sum1[index_val], meanW1[index_val])
  return meanW1[index_val], sum2[index_val], width_tr[index_val], width_te[index_val]

def calibration3(list_min, list_max, test_list_min, test_list_max, y_val, y_test, re2): #NNGU
  print(len(list_min))
  nnnum=len(list_min)
  sum1=np.zeros(len(list_min))
  sum2=np.zeros(len(test_list_min))
  list_min=np.array(list_min)
  list_max=np.array(list_max)
  test_list_min=np.array(test_list_min)
  test_list_max=np.array(test_list_max)
  W1=np.zeros([nnnum,numval])
  for tt in range(nnnum):
    for i in range(numval):
      for j in range(re2):
        if list_min[tt,i*re2+j]-margin<=y_val[i*re2+j]<=list_max[tt,i*re2+j]+margin:
          W1[tt,i]=W1[tt,i]+1/re2
  covW1=np.cov(W1)
  meanW1=np.mean(W1,-1)
  print(meanW1.shape)
  print('sum form the val', meanW1)
  sigma_diagonal = [covW1[i,i] for i in range(nnnum)]
  sigma_diagonal = np.sqrt(sigma_diagonal)+0.001
  conf_level=0.95
  mean = np.zeros(nnnum)
  cov = covW1
  samples = np.max(np.random.multivariate_normal(mean, cov, 10000), axis = 1)
  cl_quantile = np.quantile(samples, conf_level)
  print('diagonal', sigma_diagonal)
  print('cl', cl_quantile)
  for tt in range(len(sum1)):
    for i in range(len(y_val)):
      if list_min[tt,i]-margin<=y_val[i]<=list_max[tt,i]+margin:
        sum1[tt]=sum1[tt]+1/len(y_val)
  for tt in range(len(sum2)):
    for i in range(len(y_test)):
      if test_list_min[tt,i]-margin<=y_test[i]<=test_list_max[tt,i]+margin:
        sum2[tt]=sum2[tt]+1/len(y_test)
  print('sum form the val', sum1, sum2)
  width_tr = np.mean(list_max-list_min,-1)+2*margin
  width_te = np.mean(test_list_max-test_list_min,-1)+2*margin
  print('width from the val set',width_tr,width_te)
  if 0 not in list(map(int, meanW1 <= 0.95+cl_quantile/np.sqrt(numval))):
    index_val = list(meanW1).index(np.max(meanW1))
  else:
    min_value = np.min(width_tr[meanW1>0.95+cl_quantile/np.sqrt(numval)])
    index_val = list(width_tr).index(min_value)
  print('check', sum1[index_val], meanW1[index_val])
  return meanW1[index_val], sum2[index_val], width_tr[index_val], width_te[index_val]

def calibration1(list_min, list_max, test_list_min, test_list_max, y_val, y_test, re2):
  print(len(list_min))
  sum1=np.zeros(len(list_min))
  sum2=np.zeros(len(test_list_min))
  list_min=np.array(list_min)
  list_max=np.array(list_max)
  test_list_min=np.array(test_list_min)
  test_list_max=np.array(test_list_max)
  print(sum1.shape)
  for tt in range(len(sum1)):
    for i in range(len(y_val)):
      if list_min[tt,i]-margin<=y_val[i]<=list_max[tt,i]+margin:
        sum1[tt]=sum1[tt]+1/len(y_val)
  for tt in range(len(sum2)):
    for i in range(len(y_test)):
      if test_list_min[tt,i]-margin<=y_test[i]<=test_list_max[tt,i]+margin:
        sum2[tt]=sum2[tt]+1/len(y_test)
  print('sum form the val', sum1, sum2)
  width_tr = np.mean(list_max-list_min,-1)+2*margin
  width_te = np.mean(test_list_max-test_list_min,-1)+2*margin
  print('width from the val set',width_tr,width_te)
  if np.max(sum1) <= 0.95:
    index_val = list(sum1).index(np.max(sum1))
  else:
    min_value = np.min(width_tr[sum1>0.95])
    index_val = list(width_tr).index(min_value)
  return sum1[index_val], sum2[index_val], width_tr[index_val], width_te[index_val]

num_re=3
num_re2=2
model_generation(re = num_re, re2 = num_re2, re3 = 100, datatr=datatr, datate=datate)

NR = 50
a11 = np.zeros(NR)
a12 = np.zeros(NR)
a13 = np.zeros(NR)
a14 = np.zeros(NR)
for i in range(NR):
  a14[i], a11[i], a13[i], a12[i] = dataset_generation1(re = num_re, re2 = num_re2, re3 = 100, i=i, datatr=datatr, datate=datate)
print('IW', np.mean(a12))
a15 = np.zeros(len(a11))
a15[a11>0.95] = 1
print('EP',np.mean(a15))

NR = 50
a11 = np.zeros(NR)
a12 = np.zeros(NR)
a13 = np.zeros(NR)
a14 = np.zeros(NR)
for i in range(NR):
  a14[i], a11[i], a13[i], a12[i] = dataset_generation2(re = num_re, re2 = num_re2, re3 = 100, i=i, datatr=datatr, datate=datate)
print('IW', np.mean(a12))
a15 = np.zeros(len(a11))
a15[a11>0.95] = 1
print('EP',np.mean(a15))

NR = 50
a11 = np.zeros(NR)
a12 = np.zeros(NR)
a13 = np.zeros(NR)
a14 = np.zeros(NR)
for i in range(NR):
  a14[i], a11[i], a13[i], a12[i] = dataset_generation3(re = num_re, re2 = num_re2, re3 = 100, i=i, datatr=datatr, datate=datate)
print('IW', np.mean(a12))
a15 = np.zeros(len(a11))
a15[a11>0.95] = 1
print('EP',np.mean(a15))